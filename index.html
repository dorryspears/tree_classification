<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="classifying_trees">
  <meta name="keywords" content="tactile sensing, audio, agriculture">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Classifying Tree Contact Events via Audio-Visual Learning</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/RI_logo.jpg">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
</nav>

<!-- 1. Hero / Landing Section -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Classifying Tree Contact Events via Audio-Visual Learning</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <span>Anonymized Authors</span>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <span>Anonymized Organization</span>
            </span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Video Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://anonymous.4open.science/r/tree_classifier-4690"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://drive.google.com/file/d/1o92c4k1DGEqw3XG91Jf97HM_2TFRe-8X/view?usp=sharing"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Hero Image -->
<section class="hero teaser"> 
  <div class="container is-max-desktop">
    <div class="hero-body">
      <figure style="text-align: center;">
          <img src="./static/images/title_nocaption.jpg" 
             class="hero-image"
             alt="Hero Image"
             style="max-width: 80%; height: auto;"/>
        <figcaption>Robotic arm in an apple orchard using a hand-held multisensory probe for data collection.</figcaption>
      </figure>
    </div>
  </div>
</section> 

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Agricultural environments are highly unstructured and variable, posing challenges for robotic manipulation systems that rely solely on visual perception. We present a multi-modal classification framework that fuses audio and visual data to identify which part of a tree—leaf, twig, trunk, or ambient—a robot is in contact with. Evaluated on real robot data containing motor noise and out-of-distribution audio, our approach achieves an <strong>F1-score of 0.74</strong>. These results demonstrate the promise of audio-visual perception for enabling safe, contact-rich manipulation in challenging agricultural settings.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<!-- 2. Problem Overview Section -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Problem Overview</h2>
        <div class="content has-text-justified">
          <p>
            Despite advances in vision-based perception, heavy occlusions and sensor noise in cluttered foliage make contact events hard to perceive. Our key insight is that contact sounds—captured via contact microphones—provide complementary information when vision fails, enabling more robust semantic reasoning about interactions (leaf vs. twig vs. trunk vs. ambient).
          </p>
        </div>
        
        <figure style="text-align: center;">
          <img src="./static/images/sensor_probe.jpg" 
               class="interpolation-image"
               alt="Sensor Probe"/>
          <figcaption>Figure 1: Multi-modal sensor suite that can be hand-held in order to facilitate
data collection.</figcaption>
        </figure>
      </div>
    </div>
  </div>
</section>

<!-- 3. Data & Segmentation Pipeline Section -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Data & Segmentation Pipeline</h2>
        <div class="content has-text-justified">
          <p>
            We automate annotation by computing a smoothed moving average of audio amplitudes and dynamically determining thresholds to separate contact from non-contact intervals. Segments shorter than 1 s are discarded, and gaps below a merge threshold are merged, producing high-quality labels without manual effort.
          </p>
          <p>
            <strong>Key equations:</strong>
          </p>
          <p style="text-align: center;">
            \[
            T_{\rm dynamic} = F_{\rm noise} + (F_{\rm signal} - F_{\rm noise})\,\alpha_{\rm offset}
            \quad,\quad
            C(t) = 
            \begin{cases}
            1 & E(t)>T_{\rm dynamic}\\
            0 & E(t)\le T_{\rm dynamic}
            \end{cases}
            \]
          </p>
        </div>
        
        <figure style="text-align: center;">
          <img src="./static/images/segmentation.png" 
               class="interpolation-image"
               alt="Segmentation Pipeline"/>
          <figcaption>Figure 4: Audio-based automatic segmentation of contact vs. ambient intervals.</figcaption>
        </figure>
      </div>
    </div>
  </div>
</section>

<!-- 4. Model Architecture Section -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Model Architecture</h2>
        <div class="content has-text-justified">
          <p>
            Our multimodal classifier encodes mel-spectrograms via an Audio Spectrogram Transformer (AST) and a CLAP encoder, and RGB images via a Vision Transformer (ViT). A self-attention transformer fuses these embeddings, followed by an MLP head for final contact-class predictions.
          </p>
        </div>
        
        <figure style="text-align: center;">
          <img src="./static/images/system_diagram.jpg" 
               class="interpolation-image"
               alt="Model Architecture"/>
          <figcaption>Figure 6: Overall system diagram. Audio and image features are fused through a transformer encoder.</figcaption>
        </figure>
      </div>
    </div>
  </div>
</section>

<!-- 5. Quantitative Results Section -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Quantitative Results</h2>
        <div class="content has-text-justified">
          <p>
            When trained on human probe data and tested on robot recordings, our fused audio-visual model achieves a multiclass <strong>F1-score of 0.74</strong> (Precision 0.75, Recall 0.73), outperforming audio-only (0.53) and image-only (0.55) baselines. Binary contact detection reaches <strong>F1 = 0.90</strong>.
          </p>
        </div>
        
        <div class="columns">
          <div class="column">
            <figure style="text-align: center;">
              <img src="./static/images/confusion_matrix.png" 
                   class="interpolation-image"
                   alt="Multiclass Results"/>
              <figcaption>Figure 7: Confusion matrix of multiclass classification resultsjjjj.</figcaption>
            </figure>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- 6. Qualitative Visualization Section -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Qualitative Visualization</h2>
        <div class="content has-text-justified">
          <p>
            T-SNE projections of audio embeddings reveal clear separation of ambient vs. leaf, with some overlap between twig and trunk on probe data. Robot data clusters are less distinct, underscoring the need for multisensory fusion.
          </p>
        </div>
        
        <figure style="text-align: center;">
          <img src="./static/images/tsne.png" 
               class="interpolation-image"
               alt="TSNE Visualizations"/>
          <figcaption>Figure 8: T-SNE of audio features from probe (left) and robot (right) datasets.</figcaption>
        </figure>
      </div>
    </div>
  </div>
</section>

<!-- 7. Conclusion & Next Steps Section -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Conclusion & Next Steps</h2>
        <div class="content has-text-justified">
          <p>
            This work demonstrates that contact microphones, when paired with vision, significantly improve classification of tree contacts under real-world conditions. Future work will explore real-time planning integration and extension to other agricultural contexts.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This site was created from Nerfie's template. Thanks to Keunhong Park.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
